{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c04e19",
   "metadata": {
    "papermill": {
     "duration": 0.016043,
     "end_time": "2023-12-15T09:51:35.082139",
     "exception": false,
     "start_time": "2023-12-15T09:51:35.066096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Notebook 3: Logistic Regression with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20fdd80",
   "metadata": {
    "papermill": {
     "duration": 0.014718,
     "end_time": "2023-12-15T09:51:35.112305",
     "exception": false,
     "start_time": "2023-12-15T09:51:35.097587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Little Introduction before we go on. Myself Pathik Viramgama (Pr0fess0r) and am doing a 75 days kaggle challenge where I will try rise up to as much high in rank as possible in a total of 300 hours (4 hours per aday). So upvote! What we are going to do here is to see what is the mathematical aspect that goes behind in Logsitic Regression with two categories. This is the **third** notebook in the series of scratch coding. We will progress in the difficulty as we go ahead and will surely cover all the basic ML codes from scratch to the deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b0296",
   "metadata": {
    "papermill": {
     "duration": 0.015927,
     "end_time": "2023-12-15T09:51:35.143382",
     "exception": false,
     "start_time": "2023-12-15T09:51:35.127455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Dataset \n",
    "### and Legen......wait for it......dary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d84a963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:35.176275Z",
     "iopub.status.busy": "2023-12-15T09:51:35.175832Z",
     "iopub.status.idle": "2023-12-15T09:51:36.042047Z",
     "shell.execute_reply": "2023-12-15T09:51:36.040924Z"
    },
    "papermill": {
     "duration": 0.885574,
     "end_time": "2023-12-15T09:51:36.044417",
     "exception": false,
     "start_time": "2023-12-15T09:51:35.158843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1055f",
   "metadata": {
    "papermill": {
     "duration": 0.014773,
     "end_time": "2023-12-15T09:51:36.074367",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.059594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So we are going to step into the second category of machine learning which is classification algorithms. We will start with the most basic one which is logistic regression. Yes regression can also be used to classify stuff. We will be using a heart disease prediction model that can tell us from a certain data, that a person has a heart disease or not. It has two classification, heart has disease or not. We will be using this dataset for all our binary classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8730554f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:36.106883Z",
     "iopub.status.busy": "2023-12-15T09:51:36.106050Z",
     "iopub.status.idle": "2023-12-15T09:51:36.153781Z",
     "shell.execute_reply": "2023-12-15T09:51:36.152978Z"
    },
    "papermill": {
     "duration": 0.065936,
     "end_time": "2023-12-15T09:51:36.155748",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.089812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..\\Data\\heart_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f365a0d",
   "metadata": {
    "papermill": {
     "duration": 0.014898,
     "end_time": "2023-12-15T09:51:36.186339",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.171441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabc4e4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:36.219144Z",
     "iopub.status.busy": "2023-12-15T09:51:36.218527Z",
     "iopub.status.idle": "2023-12-15T09:51:36.233191Z",
     "shell.execute_reply": "2023-12-15T09:51:36.232125Z"
    },
    "papermill": {
     "duration": 0.034161,
     "end_time": "2023-12-15T09:51:36.235542",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.201381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df.drop(['target'], axis = 1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4218bbc",
   "metadata": {
    "papermill": {
     "duration": 0.072845,
     "end_time": "2023-12-15T09:51:36.323972",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.251127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to use the same methodology we used in Linear Regression using Gradient Descent. If you haven't checked it out, go check it out. \n",
    "PS: Telling you to check it out because I want upvotes on that too, not because you will need any of that here since I will be explaining from scratch here too. Although this is one level up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21c5923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:36.357267Z",
     "iopub.status.busy": "2023-12-15T09:51:36.356512Z",
     "iopub.status.idle": "2023-12-15T09:51:36.362404Z",
     "shell.execute_reply": "2023-12-15T09:51:36.361624Z"
    },
    "papermill": {
     "duration": 0.024751,
     "end_time": "2023-12-15T09:51:36.364405",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.339654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting the dataframe to numpy array so that all our data becomes in one format\n",
    "# I chose numpy array because it is faster\n",
    "x = pd.DataFrame(x).to_numpy()\n",
    "y = pd.DataFrame(y).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0900dc",
   "metadata": {
    "papermill": {
     "duration": 0.01502,
     "end_time": "2023-12-15T09:51:36.394913",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.379893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will now scale the features because it will not give us good results if used raw. Logistic regression has a sensitive hypothesis function and large differences can confuse it. You can check out my scaling notebook in my profile [here](https://www.kaggle.com/pathikviramgama/code)  to get into more detail of what and why I am doing what I am doing. For now I copy pasted the code from there to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf26077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:36.428276Z",
     "iopub.status.busy": "2023-12-15T09:51:36.427147Z",
     "iopub.status.idle": "2023-12-15T09:51:37.656354Z",
     "shell.execute_reply": "2023-12-15T09:51:37.655186Z"
    },
    "papermill": {
     "duration": 1.248522,
     "end_time": "2023-12-15T09:51:37.658969",
     "exception": false,
     "start_time": "2023-12-15T09:51:36.410447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d37d0",
   "metadata": {
    "papermill": {
     "duration": 0.015251,
     "end_time": "2023-12-15T09:51:37.691379",
     "exception": false,
     "start_time": "2023-12-15T09:51:37.676128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Yeah I know I am cheating, using a library. But hear me out please. This notebook needs you to focus on how the Linear Regression works with Gradient Descent, not how split occurs. But I have created that pre processing stuff from scratch in another notebook. You can check that out in my profile [here](https://www.kaggle.com/pathikviramgama/code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7892ed74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:37.724899Z",
     "iopub.status.busy": "2023-12-15T09:51:37.724488Z",
     "iopub.status.idle": "2023-12-15T09:51:37.834935Z",
     "shell.execute_reply": "2023-12-15T09:51:37.833775Z"
    },
    "papermill": {
     "duration": 0.130316,
     "end_time": "2023-12-15T09:51:37.837370",
     "exception": false,
     "start_time": "2023-12-15T09:51:37.707054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Doing a train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ba22e",
   "metadata": {
    "papermill": {
     "duration": 0.015195,
     "end_time": "2023-12-15T09:51:37.868276",
     "exception": false,
     "start_time": "2023-12-15T09:51:37.853081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression with Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa96e38",
   "metadata": {
    "papermill": {
     "duration": 0.01491,
     "end_time": "2023-12-15T09:51:37.898582",
     "exception": false,
     "start_time": "2023-12-15T09:51:37.883672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we will simply train the model with our data and see what is the accuracy for a reference for our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "988cd8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:37.931277Z",
     "iopub.status.busy": "2023-12-15T09:51:37.930435Z",
     "iopub.status.idle": "2023-12-15T09:51:38.047527Z",
     "shell.execute_reply": "2023-12-15T09:51:38.046145Z"
    },
    "papermill": {
     "duration": 0.136343,
     "end_time": "2023-12-15T09:51:38.050222",
     "exception": false,
     "start_time": "2023-12-15T09:51:37.913879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pathi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train)\n",
    "LR_preds = LR.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_test,LR_preds)\n",
    "\n",
    "print(\"Accuracy: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dbb60",
   "metadata": {
    "papermill": {
     "duration": 0.015323,
     "end_time": "2023-12-15T09:51:38.081449",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.066126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression from Scratch with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ade33",
   "metadata": {
    "papermill": {
     "duration": 0.015293,
     "end_time": "2023-12-15T09:51:38.113060",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.097767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Forget Machine Learning and Math. Imagine you have a class of students and you need to create three teams. One of stupids, one of smart assess and one of average. What will you need? Some data that can reflect the knowledge, IQ, awareness of mind, responses to situation, etc of each student right? Lets say we have that data. Now what? Now you decide who is Sheldon and who is Georgie, based on some weighted function of each of qualities above. And thats it. This is literally what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa19180",
   "metadata": {
    "papermill": {
     "duration": 0.015752,
     "end_time": "2023-12-15T09:51:38.144689",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.128937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In Logistic Regression we do same things as we do in a normal regression. We intitialize parameters for each feature and a bias parameter. We create a linear equation with it but multipling features with respective parameters and adding them up. We pass this equation in a function called sigmoid to create our prediction or normally called Hypothesis Function. Then we run the gradient descent and adjust the parameters. Voila we are ready with our Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7469e",
   "metadata": {
    "papermill": {
     "duration": 0.015579,
     "end_time": "2023-12-15T09:51:38.176661",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.161082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So lets begin with the first step. We need to get the parameters up and running. So we simply initialize them as a zero vector. Again I will keep the bias parameter seperate from the other ones to better explain what is happening. The length of weight vector will be equal to number of features. Each cell gets a weight value. \n",
    "\n",
    "For our case each feature gets one parameter, making a total of 4 weight parameters. And another bias parameter is also there don't forget that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462c38e",
   "metadata": {
    "papermill": {
     "duration": 0.015522,
     "end_time": "2023-12-15T09:51:38.207817",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.192295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07f0343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.241081Z",
     "iopub.status.busy": "2023-12-15T09:51:38.240706Z",
     "iopub.status.idle": "2023-12-15T09:51:38.245956Z",
     "shell.execute_reply": "2023-12-15T09:51:38.244806Z"
    },
    "papermill": {
     "duration": 0.024551,
     "end_time": "2023-12-15T09:51:38.248230",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.223679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initializing parameters\n",
    "def initParameters(x):\n",
    "    weights = np.zeros((len(x[0]), 1))\n",
    "    bias = 0\n",
    "    return weights, bias\n",
    "\n",
    "# we will not call it seperately out here. We will call it inside the training phase itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e75d5",
   "metadata": {
    "papermill": {
     "duration": 0.015012,
     "end_time": "2023-12-15T09:51:38.278895",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.263883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let me tell you the significance of a acivation function. Our hypothesis gives us an output that is of any float value. But our answer is a cateogory. Now depending upon number and type of categories, we use these functions. See my other notebook for a full definition and explanation on activation function. Here is the link (TBD).\n",
    "\n",
    "We here have a two category classification of either heart has disease or not. 0 or 1. Do or die. All in. Now I can twist that into saying if my prediction is below 0.5 we will give it 0 else we give it 1. That's exactly what we are going to do. So we need to convert all our prediction float values into 0 and 1 while maintaining the importance of the value. Best method to do that is a sigmoid function.\n",
    "\n",
    "So first thing we require is a sigmoid function. Now what the hell is a sigmoid? It is literally this formula:\n",
    "\n",
    "$\\LARGE \\sigma (a) = \\frac{1}{1+e^{-a}}$\n",
    "\n",
    "Lets create that function that we can call later on anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7eadd10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.311792Z",
     "iopub.status.busy": "2023-12-15T09:51:38.311368Z",
     "iopub.status.idle": "2023-12-15T09:51:38.316392Z",
     "shell.execute_reply": "2023-12-15T09:51:38.315235Z"
    },
    "papermill": {
     "duration": 0.024133,
     "end_time": "2023-12-15T09:51:38.318551",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.294418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sigmoid Function for hypothesis\n",
    "def sigmoid(x):\n",
    "    sgnX = 1 / (1 + np.exp(-x))\n",
    "    return sgnX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feff936",
   "metadata": {
    "papermill": {
     "duration": 0.01518,
     "end_time": "2023-12-15T09:51:38.349401",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.334221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "But wait you thought this sigmoid is our hypothesis? Huh, you have learnt nothing. It is simply a term inside our hypothesis function that converts those big float values into range of 0 to 1! Here is the actual hypothesis function:\n",
    "\n",
    "$\\LARGE h_{(\\theta,b)}(x) = \\sigma(\\bar{x}\\cdot\\overrightarrow{W} + b)$\n",
    "\n",
    "Looks complex? Wait till you see the cost function. Its gonna blow you out of water and make you wipe the floor with youself. But for now lets create a hypothesis function using sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66aaf8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.382489Z",
     "iopub.status.busy": "2023-12-15T09:51:38.382103Z",
     "iopub.status.idle": "2023-12-15T09:51:38.387306Z",
     "shell.execute_reply": "2023-12-15T09:51:38.386181Z"
    },
    "papermill": {
     "duration": 0.024415,
     "end_time": "2023-12-15T09:51:38.389320",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.364905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hypothesis Function used for getting prediction\n",
    "def hypothesisFunction(x, weights, bias):\n",
    "    h = sigmoid(np.dot(x, weights) + bias)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d1bd9f",
   "metadata": {
    "papermill": {
     "duration": 0.01579,
     "end_time": "2023-12-15T09:51:38.421067",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.405277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to calcualte the cost function. That is simply going to tell us error in our 'hypothesis' (predictions). But yeah that error function is very massive and uses a lot of terms. Here is the formula for it:\n",
    "\n",
    "$\\LARGE J = -\\frac{1}{m}\\sum_{i=1}^{m}[\\{y_{i}*log(h_{(\\theta,b)}(x_{i}))\\} + \\{(1-y_{i})*log(1-h_{(\\theta,b)}(x_{i}))\\}]$\n",
    "\n",
    "Mesmerizing isn't it? Or you quit here? If it is any condolences, you are not a regression virgin anymore. Let's implement this in good old python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a18e94f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.456895Z",
     "iopub.status.busy": "2023-12-15T09:51:38.456446Z",
     "iopub.status.idle": "2023-12-15T09:51:38.464009Z",
     "shell.execute_reply": "2023-12-15T09:51:38.462953Z"
    },
    "papermill": {
     "duration": 0.033402,
     "end_time": "2023-12-15T09:51:38.470106",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.436704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def costFunction(x, y, weights, bias):\n",
    "    m = len(y) # number of examples or rows in our dataset\n",
    "    h = hypothesisFunction(x, weights, bias)# calculating the sigmoid function\n",
    "    cost = -(np.sum( (y*np.log(h)) + ((1 - y) * np.log(1 - h))))/m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38914cac",
   "metadata": {
    "papermill": {
     "duration": 0.021217,
     "end_time": "2023-12-15T09:51:38.510407",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.489190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let me give a heads up. Its gonna get ugly. We are going to differentiate the equation above with respect to both $\\overrightarrow{W}$ and $b$ one by one. Put in the current example values and update the current values of $\\overrightarrow{W}$ and $b$. That is where learning rate comes into action. Here is a simple intuition for learning rate and why we need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2a033",
   "metadata": {
    "papermill": {
     "duration": 0.02092,
     "end_time": "2023-12-15T09:51:38.548989",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.528069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Suppose you went up a hill and lost you keys on the way up. Now you need to go back and find it. Here are two ways you can go around that problem. Take one step down the hill and look around for the key, repeat untill found. Another approach would be take 30 steps and look around for key, repeat untill found. You would think eh we should go with the second one it would be faster. But let me add to it that in the second case there are chances you might miss the key as you are taking too many steps at a time. The problem with first one is that it is extremely slow. So we need an optimized way of searching, which according to me is around 7 steps. Getting my point, eh? \n",
    "\n",
    "Now the number of steps is another variable called the Learning Rate $\\alpha$. This decides how fast an algorithm learns. I personally set it at 0.01. Oh and it is always between 1 and 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcef982",
   "metadata": {
    "papermill": {
     "duration": 0.019235,
     "end_time": "2023-12-15T09:51:38.587565",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.568330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So lets do the Learning in Machine Learning. As we know we have calculus here. Here are the update formulas we will be using:\n",
    "\n",
    "$\\LARGE tempW = \\overrightarrow{W} - \\frac{\\alpha}{m}(x^{T}\\cdot h_{(\\theta,b)}(x))$\n",
    "\n",
    "$\\LARGE tempb = b - \\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_{(\\theta,b)}(x_{i}) - y_{i})$\n",
    "\n",
    "$\\LARGE \\overrightarrow{W} = tempW$\n",
    "\n",
    "$\\LARGE b = tempb$\n",
    "\n",
    "Now you idiots would be asking why is the update formula in two parts. Here is why. You have W = [1,1] and b = 1. Lets do the first update and directly assign the value to variable. So suppose we have the updated answer as W = [0,0], but b still remains 1 as we have not updated it yet. Now when we try to update the value of b, we will not be using the original value [1,1] of W. Instead we will be using the new value [0,0] which defeats the purpose of simultaneous updatation in gradient descent. Hence we get the all the new update values first then assign them. So lets go implement it in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38491b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.620770Z",
     "iopub.status.busy": "2023-12-15T09:51:38.620357Z",
     "iopub.status.idle": "2023-12-15T09:51:38.626093Z",
     "shell.execute_reply": "2023-12-15T09:51:38.624982Z"
    },
    "papermill": {
     "duration": 0.024931,
     "end_time": "2023-12-15T09:51:38.628198",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.603267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The learning in machine learning, Gradient Descent Algorithm\n",
    "def gradientDescent(x, y, weights, bias, alpha):\n",
    "    \n",
    "    m = len(y)\n",
    "    h = hypothesisFunction(x, weights, bias)\n",
    "    \n",
    "    dw = (1/m) * np.dot(x.T, (h - y))\n",
    "    db = (1/m) * np.sum(h - y)\n",
    "\n",
    "    weights -= alpha * dw\n",
    "    bias -= alpha * db\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "# Huh now was it so hard? Thanks to numpy it is hiding that ugly vector and matrix math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c9a228",
   "metadata": {
    "papermill": {
     "duration": 0.015463,
     "end_time": "2023-12-15T09:51:38.660127",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.644664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now ladies and gentlemen, we are here. Watch the disaster unfold as I encapsulate above functions and differentiation together and create Logistic Regression using Gradient Descent. We will use everything we defined above and run it over certain number of times over and over again to get perfect parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3641a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.694685Z",
     "iopub.status.busy": "2023-12-15T09:51:38.694315Z",
     "iopub.status.idle": "2023-12-15T09:51:38.699200Z",
     "shell.execute_reply": "2023-12-15T09:51:38.698520Z"
    },
    "papermill": {
     "duration": 0.024435,
     "end_time": "2023-12-15T09:51:38.701260",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.676825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combining all the above functions together\n",
    "def train(x, y, alpha, epochs):\n",
    "    \n",
    "    # initialize the paramters\n",
    "    weights, bias = initParameters(x)\n",
    "    \n",
    "    # loop for number of iterations\n",
    "    for i in range(epochs):\n",
    "        # get the cost\n",
    "        cost = costFunction(x, y, weights, bias)\n",
    "        # run the learning algorithm that updates the values\n",
    "        weights, bias = gradientDescent(x, y, weights, bias, alpha)\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cfef1a",
   "metadata": {
    "papermill": {
     "duration": 0.015156,
     "end_time": "2023-12-15T09:51:38.732381",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.717225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you still dont get this one, don't think 'logistic regression isn't for you'. Please think 'you are not made for logistic regression', you should rather be an influencer on social media, easy money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12eb34",
   "metadata": {
    "papermill": {
     "duration": 0.015431,
     "end_time": "2023-12-15T09:51:38.763846",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.748415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd0ca2",
   "metadata": {
    "papermill": {
     "duration": 0.015379,
     "end_time": "2023-12-15T09:51:38.794973",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.779594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets create the predictions based on these new parameters. Prediction again is done using hypothesis function. We will now do that segmentation we talked about above of 0.5 above and below. We hard code that if hypothesis is below 0.5 its category a and above then category b. Lets code that into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14908b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.828606Z",
     "iopub.status.busy": "2023-12-15T09:51:38.828186Z",
     "iopub.status.idle": "2023-12-15T09:51:38.832715Z",
     "shell.execute_reply": "2023-12-15T09:51:38.831875Z"
    },
    "papermill": {
     "duration": 0.024151,
     "end_time": "2023-12-15T09:51:38.834866",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.810715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(x, weights, bias):\n",
    "    predictions = (hypothesisFunction(x, weights, bias) >= 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987f833",
   "metadata": {
    "papermill": {
     "duration": 0.015864,
     "end_time": "2023-12-15T09:51:38.868439",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.852575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ...And Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26a62963",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:38.902124Z",
     "iopub.status.busy": "2023-12-15T09:51:38.901459Z",
     "iopub.status.idle": "2023-12-15T09:51:38.971051Z",
     "shell.execute_reply": "2023-12-15T09:51:38.970061Z"
    },
    "papermill": {
     "duration": 0.089374,
     "end_time": "2023-12-15T09:51:38.973494",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.884120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "weights, bias = train(x_train, y_train, alpha=0.01, epochs=1000)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = predict(x_test, weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20415da",
   "metadata": {
    "papermill": {
     "duration": 0.015366,
     "end_time": "2023-12-15T09:51:39.004934",
     "exception": false,
     "start_time": "2023-12-15T09:51:38.989568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862f2ef",
   "metadata": {
    "papermill": {
     "duration": 0.015768,
     "end_time": "2023-12-15T09:51:39.037723",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.021955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now lets evaluate our algorithm. For classification we have multiple things to rate our model. Unlike Regression, clssificaation has accuracy. But it has a hundered more things. Now just for the purpose of harasing you I will include as many as I can. However please note that you might only need a handful of these, and library functions for each of them is available too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ea3af",
   "metadata": {
    "papermill": {
     "duration": 0.016101,
     "end_time": "2023-12-15T09:51:39.070143",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.054042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Basic Evaluation Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8466009",
   "metadata": {
    "papermill": {
     "duration": 0.016699,
     "end_time": "2023-12-15T09:51:39.102958",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.086259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First we will understand how many different answer our model can give.\n",
    "1. Model predicted 0 and in reality it is 0. These predictions are called true negatives.\n",
    "2. Model predicted 0 but in reality it is 1. These predictions are called false negatives\n",
    "3. Model predicted 1 and in reality it is 1. These predictions are called true positives.\n",
    "4. Model predicted 1 but in reality it is 0. These predictions are called false positives.\n",
    "\n",
    "Now since this is scratch coding, we will code this too from scratch. Let's code in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b207bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.137733Z",
     "iopub.status.busy": "2023-12-15T09:51:39.137302Z",
     "iopub.status.idle": "2023-12-15T09:51:39.144337Z",
     "shell.execute_reply": "2023-12-15T09:51:39.142999Z"
    },
    "papermill": {
     "duration": 0.02728,
     "end_time": "2023-12-15T09:51:39.146456",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.119176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findConfusionTerms(y, y_pred):\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == 0 and y[i] == 0):\n",
    "            TN += 1\n",
    "        if(y_pred[i] == 0 and y[i] == 1):\n",
    "            FN += 1\n",
    "        if(y_pred[i] == 1 and y[i] == 1):\n",
    "            TP += 1 \n",
    "        if(y_pred[i] == 1 and y[i] == 0):\n",
    "            FP += 1\n",
    "    return TN, FN, TP, FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d2d92",
   "metadata": {
    "papermill": {
     "duration": 0.016794,
     "end_time": "2023-12-15T09:51:39.179946",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.163152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we calculate what is called a confusion matrix! It is basically a visual representaion of these terms in a better way! Now I am no graphic designer so I am just creating a simple array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a2dfcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.223038Z",
     "iopub.status.busy": "2023-12-15T09:51:39.221979Z",
     "iopub.status.idle": "2023-12-15T09:51:39.227296Z",
     "shell.execute_reply": "2023-12-15T09:51:39.226515Z"
    },
    "papermill": {
     "duration": 0.030191,
     "end_time": "2023-12-15T09:51:39.230237",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.200046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findConfusionMatrix(TN, FN, TP, FP):\n",
    "    CM = [[TN, FP],[FN, TP]]\n",
    "    return CM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2910a6",
   "metadata": {
    "papermill": {
     "duration": 0.018999,
     "end_time": "2023-12-15T09:51:39.271101",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.252102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now before we go on, I will tell a story. Scientist created what I coded above. Now the model was working, hence they were at a risk of getting laid off. So they decided, \"No we will not stop. We will make more terms and turn the world upside down just like Luffy will!\". Hence they created hundred more terms. Now I amd not saying they are not useful but man who is going to remember so many formulas? Anyways I will scratch code them too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc2cb2",
   "metadata": {
    "papermill": {
     "duration": 0.01723,
     "end_time": "2023-12-15T09:51:39.304493",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.287263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d19b47",
   "metadata": {
    "papermill": {
     "duration": 0.015439,
     "end_time": "2023-12-15T09:51:39.335585",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.320146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets begin with a simple Type I error, also referred as False Positive Rate. You rarely would use this metric alone.\n",
    "\n",
    "$\\LARGE FPR = \\frac{FP}{FP+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a948cca9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.368748Z",
     "iopub.status.busy": "2023-12-15T09:51:39.368334Z",
     "iopub.status.idle": "2023-12-15T09:51:39.373160Z",
     "shell.execute_reply": "2023-12-15T09:51:39.372449Z"
    },
    "papermill": {
     "duration": 0.024038,
     "end_time": "2023-12-15T09:51:39.375210",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.351172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def falsePositiveRate(FP, TN):\n",
    "    FPR = FP/(FP+TN)\n",
    "    return FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2707b",
   "metadata": {
    "papermill": {
     "duration": 0.015466,
     "end_time": "2023-12-15T09:51:39.406353",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.390887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## False Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b019f01d",
   "metadata": {
    "papermill": {
     "duration": 0.015265,
     "end_time": "2023-12-15T09:51:39.437276",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.422011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next is Type II error, commonly called False Negative Rate. Usually, it is not used alone but rather with some other metric.\n",
    "\n",
    "$\\LARGE FNR = \\frac{FN}{FN+TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf59b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.471049Z",
     "iopub.status.busy": "2023-12-15T09:51:39.470253Z",
     "iopub.status.idle": "2023-12-15T09:51:39.475775Z",
     "shell.execute_reply": "2023-12-15T09:51:39.474778Z"
    },
    "papermill": {
     "duration": 0.024859,
     "end_time": "2023-12-15T09:51:39.477957",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.453098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def falseNegativeRate(FN, TN):\n",
    "    FNR = FN/(FN+TN)\n",
    "    return FNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544e3cc",
   "metadata": {
    "papermill": {
     "duration": 0.01534,
     "end_time": "2023-12-15T09:51:39.509192",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.493852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5c6a5",
   "metadata": {
    "papermill": {
     "duration": 0.015318,
     "end_time": "2023-12-15T09:51:39.540365",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.525047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now a good one. We have Specificity, commonly known as True Negative Rate. This is used more often. Formula for it is:\n",
    "\n",
    "$\\LARGE TNR = \\frac{TN}{TN+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0277caf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.573059Z",
     "iopub.status.busy": "2023-12-15T09:51:39.572687Z",
     "iopub.status.idle": "2023-12-15T09:51:39.577794Z",
     "shell.execute_reply": "2023-12-15T09:51:39.576688Z"
    },
    "papermill": {
     "duration": 0.023833,
     "end_time": "2023-12-15T09:51:39.579770",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.555937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trueNegativeRate(TN, FP):\n",
    "    TNR = TN/(TN+FP)\n",
    "    return TNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0295b",
   "metadata": {
    "papermill": {
     "duration": 0.015552,
     "end_time": "2023-12-15T09:51:39.611288",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.595736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Negative Prediction Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5589c97",
   "metadata": {
    "papermill": {
     "duration": 0.015361,
     "end_time": "2023-12-15T09:51:39.643049",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.627688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is one I did not know existed. Formula for it:\n",
    "\n",
    "$\\LARGE NPV = \\frac{TN}{TN+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c0d8f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.676733Z",
     "iopub.status.busy": "2023-12-15T09:51:39.675986Z",
     "iopub.status.idle": "2023-12-15T09:51:39.680136Z",
     "shell.execute_reply": "2023-12-15T09:51:39.679441Z"
    },
    "papermill": {
     "duration": 0.023326,
     "end_time": "2023-12-15T09:51:39.682018",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.658692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def negativePredictionValue(TN, FN):\n",
    "    NPV = TN/(TN+FN)\n",
    "    return NPV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75dd5e",
   "metadata": {
    "papermill": {
     "duration": 0.015104,
     "end_time": "2023-12-15T09:51:39.713168",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.698064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## False Discovery Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7e707",
   "metadata": {
    "papermill": {
     "duration": 0.01533,
     "end_time": "2023-12-15T09:51:39.744044",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.728714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is also one I did not know existed. Formula for it:\n",
    "\n",
    "$\\LARGE TNR = \\frac{FP}{FP+TP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be37bad6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.777658Z",
     "iopub.status.busy": "2023-12-15T09:51:39.776954Z",
     "iopub.status.idle": "2023-12-15T09:51:39.781555Z",
     "shell.execute_reply": "2023-12-15T09:51:39.780517Z"
    },
    "papermill": {
     "duration": 0.024065,
     "end_time": "2023-12-15T09:51:39.783722",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.759657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def falseDiscoveryRate(FP, TP):\n",
    "    FDR = FP/(FP+TP)\n",
    "    return FDR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa578dc6",
   "metadata": {
    "papermill": {
     "duration": 0.016656,
     "end_time": "2023-12-15T09:51:39.820426",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.803770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e0edd",
   "metadata": {
    "papermill": {
     "duration": 0.015766,
     "end_time": "2023-12-15T09:51:39.852378",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.836612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next up is a important one, Recall or Sensitivity also called True Positive Rate. Formula for it is:\n",
    "\n",
    "$\\LARGE TPR = \\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "753e2993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.887235Z",
     "iopub.status.busy": "2023-12-15T09:51:39.886838Z",
     "iopub.status.idle": "2023-12-15T09:51:39.891555Z",
     "shell.execute_reply": "2023-12-15T09:51:39.890684Z"
    },
    "papermill": {
     "duration": 0.023393,
     "end_time": "2023-12-15T09:51:39.893392",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.869999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def truePositiveRate(TP, FN):\n",
    "    TPR = TP/(TP+FN)\n",
    "    return TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04254c",
   "metadata": {
    "papermill": {
     "duration": 0.015356,
     "end_time": "2023-12-15T09:51:39.924589",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.909233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af35d8",
   "metadata": {
    "papermill": {
     "duration": 0.015406,
     "end_time": "2023-12-15T09:51:39.956328",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.940922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now is an important one. Positive Prediction Value or generally called Precision. Formula for it is:\n",
    "\n",
    "$\\LARGE PPV = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7a4a805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:39.989762Z",
     "iopub.status.busy": "2023-12-15T09:51:39.988677Z",
     "iopub.status.idle": "2023-12-15T09:51:39.994134Z",
     "shell.execute_reply": "2023-12-15T09:51:39.993111Z"
    },
    "papermill": {
     "duration": 0.024203,
     "end_time": "2023-12-15T09:51:39.996330",
     "exception": false,
     "start_time": "2023-12-15T09:51:39.972127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def positivePredictionValue(TP, FP):\n",
    "    PPV = TP/(TP+FP)\n",
    "    return PPV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cd2af",
   "metadata": {
    "papermill": {
     "duration": 0.015567,
     "end_time": "2023-12-15T09:51:40.028090",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.012523",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50498118",
   "metadata": {
    "papermill": {
     "duration": 0.015415,
     "end_time": "2023-12-15T09:51:40.059359",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.043944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And Finally we have Accuracy. Correct divided by total. Formual for it is:\n",
    "\n",
    "\n",
    "$\\LARGE ACC = \\frac{TP + TN}{TP + FP + TN +FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a0819b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.092276Z",
     "iopub.status.busy": "2023-12-15T09:51:40.091883Z",
     "iopub.status.idle": "2023-12-15T09:51:40.096211Z",
     "shell.execute_reply": "2023-12-15T09:51:40.095412Z"
    },
    "papermill": {
     "duration": 0.023082,
     "end_time": "2023-12-15T09:51:40.098035",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.074953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(TN, FN, TP, FP):\n",
    "    acc = (TP + TN)/(TP+FP+TN+FN)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e319a6",
   "metadata": {
    "papermill": {
     "duration": 0.016042,
     "end_time": "2023-12-15T09:51:40.130501",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.114459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## F$_\\beta$ Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf2bdf",
   "metadata": {
    "papermill": {
     "duration": 0.016,
     "end_time": "2023-12-15T09:51:40.162952",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.146952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a general term where we change the value of $\\beta$ and create different values. Formula for it:\n",
    "\n",
    "$\\LARGE F_{\\beta} = (1+\\beta^2)\\frac{PPV*TPR}{\\beta^2*PPV + TPR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a9c133b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.197114Z",
     "iopub.status.busy": "2023-12-15T09:51:40.196045Z",
     "iopub.status.idle": "2023-12-15T09:51:40.201816Z",
     "shell.execute_reply": "2023-12-15T09:51:40.200805Z"
    },
    "papermill": {
     "duration": 0.025117,
     "end_time": "2023-12-15T09:51:40.203888",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.178771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def FBetaScore(PPV, TPR, beta):\n",
    "    FBeta = (1 + beta**2)*PPV*TPR/((beta**2*PPV)+TPR)\n",
    "    return FBeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8bb0d",
   "metadata": {
    "papermill": {
     "duration": 0.015577,
     "end_time": "2023-12-15T09:51:40.235616",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.220039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3095ed",
   "metadata": {
    "papermill": {
     "duration": 0.01557,
     "end_time": "2023-12-15T09:51:40.267177",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.251607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Eh you thought it is over? No there is F1 score. It is a part of the beta score when beta is 1. F1 is basically the harmonic mean between precision and recall. Formula for it is:\n",
    "\n",
    "$\\LARGE  F1 = 2\\frac{PPV*TPR}{PPV+TPR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49dbb78d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.301799Z",
     "iopub.status.busy": "2023-12-15T09:51:40.300730Z",
     "iopub.status.idle": "2023-12-15T09:51:40.306716Z",
     "shell.execute_reply": "2023-12-15T09:51:40.305532Z"
    },
    "papermill": {
     "duration": 0.02597,
     "end_time": "2023-12-15T09:51:40.309222",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.283252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def F1Score(PPV, TPR):\n",
    "    F1 = FBetaScore(PPV, TPR, 1)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3da905",
   "metadata": {
    "papermill": {
     "duration": 0.016346,
     "end_time": "2023-12-15T09:51:40.341600",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.325254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## F2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc0617",
   "metadata": {
    "papermill": {
     "duration": 0.015477,
     "end_time": "2023-12-15T09:51:40.372896",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.357419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then there is F2 score. It is a part of the beta score when beta is 2. F2 is basically the harmonic mean between precision and recall. Formula for it is:\n",
    "\n",
    "$\\LARGE F2 = 5\\frac{PPV*TPR}{4*PPV+TPR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6c9e020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.406347Z",
     "iopub.status.busy": "2023-12-15T09:51:40.405982Z",
     "iopub.status.idle": "2023-12-15T09:51:40.410914Z",
     "shell.execute_reply": "2023-12-15T09:51:40.409643Z"
    },
    "papermill": {
     "duration": 0.024334,
     "end_time": "2023-12-15T09:51:40.412977",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.388643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def F2Score(PPV, TPR):\n",
    "    F2 = FBetaScore(PPV, TPR, 2)\n",
    "    return F2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a31dd",
   "metadata": {
    "papermill": {
     "duration": 0.016104,
     "end_time": "2023-12-15T09:51:40.445820",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.429716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc289b8f",
   "metadata": {
    "papermill": {
     "duration": 0.015246,
     "end_time": "2023-12-15T09:51:40.477364",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.462118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is just like accuracy as well. It’s a correlation between predicted classes and ground truth. Formula is:\n",
    "\n",
    "$\\LARGE MCC = \\frac{(TP*TN)+(FP*FN)}{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62641b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.512831Z",
     "iopub.status.busy": "2023-12-15T09:51:40.511478Z",
     "iopub.status.idle": "2023-12-15T09:51:40.517155Z",
     "shell.execute_reply": "2023-12-15T09:51:40.516376Z"
    },
    "papermill": {
     "duration": 0.024546,
     "end_time": "2023-12-15T09:51:40.519136",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.494590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def matthewsCorrelationCoefficient(TN, FN, TP, FP):\n",
    "    MCC = ((TP*TN)+(FP*FN)) / ( (TP+FP)*(TP+FN)*(TN+FP)*(TN+FN) )\n",
    "    return MCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba3559",
   "metadata": {
    "papermill": {
     "duration": 0.015347,
     "end_time": "2023-12-15T09:51:40.550134",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.534787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Full Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a36870b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.583204Z",
     "iopub.status.busy": "2023-12-15T09:51:40.582544Z",
     "iopub.status.idle": "2023-12-15T09:51:40.663183Z",
     "shell.execute_reply": "2023-12-15T09:51:40.662382Z"
    },
    "papermill": {
     "duration": 0.100038,
     "end_time": "2023-12-15T09:51:40.665610",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.565572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics(y, y_pred):\n",
    "    \n",
    "    TN, FN, TP, FP = findConfusionTerms(y, y_pred)\n",
    "    CM = findConfusionMatrix(TN, FN, TP, FP)\n",
    "    \n",
    "    FPR = falsePositiveRate(FP, TN)\n",
    "    FNR = falseNegativeRate(FN, TN)\n",
    "    TNR = trueNegativeRate(TN, FP)\n",
    "    NPV = negativePredictionValue(TN, FN)\n",
    "    FDR = falseDiscoveryRate(FP, TP)\n",
    "    TPR = truePositiveRate(TP, FN)\n",
    "    PPV = positivePredictionValue(TP, FP)\n",
    "    \n",
    "    ACC = accuracy(TN, FN, TP, FP)\n",
    "    \n",
    "    F1 = F1Score(PPV, TPR)\n",
    "    F2 = F2Score(PPV, TPR)\n",
    "    \n",
    "    MCC = matthewsCorrelationCoefficient(TN, FN, TP, FP)\n",
    "        \n",
    "    print(\"Confusion Matrix: \",CM)\n",
    "    \n",
    "    print(\"Type I Error: \", FPR)\n",
    "    print(\"Type II Error: \", FNR)\n",
    "    print(\"Specificity: \", TNR)\n",
    "    print(\"Negative Prediction Rate: \", NPV)\n",
    "    print(\"False Discovery Rate: \", FDR)\n",
    "    print(\"Recall: \", TPR)\n",
    "    print(\"Precision: \", PPV)\n",
    "    \n",
    "    print(\"Accuracy: \", ACC)\n",
    "    \n",
    "    print(\"F1 Score: \", F1)\n",
    "    print(\"F2 Score: \", F2)\n",
    "    \n",
    "    print(\"Matthews Correlation Coefficient: \", MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51760e20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-15T09:51:40.699304Z",
     "iopub.status.busy": "2023-12-15T09:51:40.698911Z",
     "iopub.status.idle": "2023-12-15T09:51:40.705023Z",
     "shell.execute_reply": "2023-12-15T09:51:40.704044Z"
    },
    "papermill": {
     "duration": 0.025617,
     "end_time": "2023-12-15T09:51:40.707455",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.681838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[33, 9], [10, 48]]\n",
      "Type I Error:  0.21428571428571427\n",
      "Type II Error:  0.23255813953488372\n",
      "Specificity:  0.7857142857142857\n",
      "Negative Prediction Rate:  0.7674418604651163\n",
      "False Discovery Rate:  0.15789473684210525\n",
      "Recall:  0.8275862068965517\n",
      "Precision:  0.8421052631578947\n",
      "Accuracy:  0.81\n",
      "F1 Score:  0.8347826086956522\n",
      "F2 Score:  0.8304498269896194\n",
      "Matthews Correlation Coefficient:  0.0002803721412593231\n"
     ]
    }
   ],
   "source": [
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cca09",
   "metadata": {
    "papermill": {
     "duration": 0.015979,
     "end_time": "2023-12-15T09:51:40.739433",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.723454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That you think as easy, but it wasn't. The logic took one-third of the time it took for this evaluation. Even I did not know these many terms existed. I had to learn stuff to explain it to you. I have not included many other terms because they deal with the threshold, and we are not yet at the expertise level to play with it. Maybe in future we will."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6483c028",
   "metadata": {
    "papermill": {
     "duration": 0.015298,
     "end_time": "2023-12-15T09:51:40.770708",
     "exception": false,
     "start_time": "2023-12-15T09:51:40.755410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "See just four lines of library code instead of all this mess we did. No wonder human coders are fat ass and lazy as a sloth. But yeah atleast now I know I am useless. There are a lot more things and stuff to manipulate in a logistic regression algorithm like regularization and hyperparameter tunning. For now we conclude this code."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1828813,
     "sourceId": 2983853,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.126544,
   "end_time": "2023-12-15T09:51:41.307294",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-15T09:51:31.180750",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
